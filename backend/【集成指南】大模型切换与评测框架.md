# 大模型切换与评测框架 - 集成指南

## 📋 概述

本文档说明如何将新开发的**大模型切换机制**和**评测框架**集成到现有的`main_db.py`中。

## 🎯 三大核心模块

### 1. config.py - 模型配置中心
- 统一管理所有模型配置
- 一键切换模型（修改`ACTIVE_MODEL_KEY`）

### 2. model_adapter.py - 模型适配器
- 统一不同模型的调用接口
- 支持Dashscope和OpenAI兼容API

### 3. evaluation_suite.py - 评测框架
- 自动记录评测数据
- 多维度评分系统
- 生成对比报告

---

## 🔧 集成步骤

### 步骤1：修改 main_db.py 的导入部分

在`main_db.py`顶部添加新模块的导入：

```python
# 【新增】大模型适配器
from model_adapter import (
    MultiModalModelAdapter,
    TextModelAdapter,
    get_multimodal_adapter,
    get_text_adapter
)

# 【新增】评测框架
from evaluation_suite import (
    create_evaluation_record,
    EvaluationLogger,
    quick_evaluate_and_log
)

# 【新增】配置
import config
```

### 步骤2：替换AI调用逻辑

#### 原代码（大约在第1630-1700行）

```python
# 原来的代码（仅示例）
response = dashscope.MultiModalConversation.call(
    model='qwen-vl-max',
    messages=messages,
    stream=True
)
```

#### 修改为（使用适配器）

```python
# 【修改】使用统一适配器
adapter = get_multimodal_adapter()  # 自动使用config.py中配置的模型

# 构建消息
messages = [
    {
        "role": "user",
        "content": [
            {"image": f"data:image/jpeg;base64,{current_image}"},
            {"text": prompt}
        ]
    }
]

# 添加历史消息
if session_id and session_id in chat_sessions:
    messages = chat_sessions[session_id]["messages"] + messages

# 调用模型
full_response = ""
start_time = time.time()

for chunk in adapter.call(messages, stream=True):
    if chunk["finish_reason"] == "error":
        # 处理错误
        error_msg = chunk.get("error", "未知错误")
        print(f"❌ AI调用失败: {error_msg}")
        break
    
    content = chunk["content"]
    full_response += content
    
    # 流式返回给前端（保持原有逻辑）
    yield f"data: {json.dumps({'content': content})}\n\n"

response_time = time.time() - start_time
```

### 步骤3：集成评测记录

在AI响应完成后，添加评测记录：

```python
# 【新增】在AI响应完成后记录评测数据
if request.mode == 'review':  # 批改模式
    task_type = 'review'
elif '生成题目' in request.prompt or '出题' in request.prompt:  # 生题模式
    task_type = 'generate'
else:  # 解题模式
    task_type = 'solve'

# 记录评测（异步执行，不阻塞主流程）
try:
    quick_evaluate_and_log(
        model_name=config.ACTIVE_MODEL_KEY,
        task_type=task_type,
        input_prompt=request.prompt,
        raw_output=full_response,
        input_image_path=None,  # 可选：保存图片路径
        response_time=response_time,
        token_count=len(full_response),  # 简化计算
        notes=""  # 可由测试人员后续添加
    )
    print(f"✅ [评测] 已记录 {config.ACTIVE_MODEL_KEY} 的 {task_type} 任务")
except Exception as eval_error:
    print(f"⚠️  [评测] 记录失败: {eval_error}")
```

### 步骤4：修改知识点提取

```python
# 原代码
extract_response = dashscope.Generation.call(
    model='qwen-turbo',
    prompt=extract_prompt
)

# 【修改为】
text_adapter = get_text_adapter()
knowledge_text = text_adapter.call(extract_prompt)

if knowledge_text:
    knowledge_points = [kp.strip() for kp in knowledge_text.split('，')]
    print(f"[知识点提取] 成功: {knowledge_points}")
```

---

## 📝 完整示例：修改 /api/db/chat 路由

以下是一个完整的修改示例：

```python
@app.post("/api/db/chat")
async def db_chat(request: ChatRequest, user: dict = Depends(get_current_user)):
    """
    AI对话接口（支持多模型切换和评测）
    
    V25.2 增强：
    - 支持多模型切换
    - 自动评测记录
    """
    import time
    import config
    from model_adapter import get_multimodal_adapter, get_text_adapter
    from evaluation_suite import quick_evaluate_and_log
    
    user_id = user["user_id"]
    mode = request.mode
    prompt = request.prompt
    session_id = request.session_id
    
    # 打印当前使用的模型
    print(f"\n{'='*60}")
    print(f"[AI对话] 用户: {user_id}")
    print(f"[AI对话] 模式: {mode}")
    print(f"[AI对话] 当前模型: {config.ACTIVE_MODEL_KEY}")
    print(f"{'='*60}\n")
    
    # 处理图片
    current_image = None
    if request.image_base64:
        current_image = request.image_base64
    elif session_id and session_id in chat_sessions:
        current_image = chat_sessions[session_id].get("image_base64")
    
    # 构建消息
    messages = []
    
    # 添加系统提示（可选）
    if mode == 'solve':
        system_msg = "你是一位专业的数学老师，擅长解答各种数学问题。"
    elif mode == 'review':
        system_msg = "你是一位经验丰富的老师，擅长批改作业并指出错误。"
    else:
        system_msg = "你是一位AI助手。"
    
    messages.append({
        "role": "system",
        "content": system_msg
    })
    
    # 添加历史消息
    if session_id and session_id in chat_sessions:
        messages.extend(chat_sessions[session_id]["messages"])
    
    # 添加当前用户消息
    user_message = {
        "role": "user",
        "content": []
    }
    
    if current_image:
        user_message["content"].append({
            "image": f"data:image/jpeg;base64,{current_image}"
        })
    
    user_message["content"].append({"text": prompt})
    messages.append(user_message)
    
    # 使用适配器调用AI
    adapter = get_multimodal_adapter()
    
    full_response = ""
    start_time = time.time()
    has_error = False
    
    async def generate():
        nonlocal full_response, has_error
        
        yield f"data: {json.dumps({'type': 'start'})}\n\n"
        
        try:
            for chunk in adapter.call(messages, stream=True):
                if chunk["finish_reason"] == "error":
                    error_msg = chunk.get("error", "未知错误")
                    yield f"data: {json.dumps({'type': 'error', 'content': error_msg})}\n\n"
                    has_error = True
                    break
                
                content = chunk["content"]
                full_response += content
                
                yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
            
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
        
        except Exception as e:
            error_msg = f"AI调用异常: {str(e)}"
            print(f"❌ {error_msg}")
            yield f"data: {json.dumps({'type': 'error', 'content': error_msg})}\n\n"
            has_error = True
    
    # 返回流式响应
    response = StreamingResponse(generate(), media_type="text/event-stream")
    
    # 【新增】在后台记录评测数据
    response_time = time.time() - start_time
    
    # 判断任务类型
    if mode == 'review':
        task_type = 'review'
    elif '生成' in prompt or '出题' in prompt:
        task_type = 'generate'
    else:
        task_type = 'solve'
    
    # 异步记录评测
    if not has_error:
        try:
            quick_evaluate_and_log(
                model_name=config.ACTIVE_MODEL_KEY,
                task_type=task_type,
                input_prompt=prompt,
                raw_output=full_response,
                response_time=response_time,
                token_count=len(full_response)
            )
        except Exception as eval_error:
            print(f"⚠️  评测记录失败: {eval_error}")
    
    # 保存会话
    if session_id:
        if session_id not in chat_sessions:
            chat_sessions[session_id] = {
                "user_id": user_id,
                "messages": [],
                "image_base64": current_image,
                "created_at": datetime.now().isoformat()
            }
        
        # 添加到历史
        chat_sessions[session_id]["messages"].append(user_message)
        chat_sessions[session_id]["messages"].append({
            "role": "assistant",
            "content": full_response
        })
    
    # 【保留】错题自动保存逻辑
    if mode == 'review' and not has_error:
        is_mistake = any(keyword in full_response for keyword in [
            '错误', '不正确', '不对', '有误', '答案错了', 
            '做错了', '有问题', '错了'
        ])
        
        if is_mistake and current_image:
            # ... 保留原有的错题保存逻辑 ...
            print(f"✅ 检测到错误，正在保存到错题本...")
    
    return response
```

---

## 🎨 模型切换演示

### 切换到32B-Instruct模型

1. 编辑 `backend/config.py`：
```python
# ACTIVE_MODEL_KEY = "qwen-vl-max"  # 注释掉原来的
ACTIVE_MODEL_KEY = "qwen3-vl-32b-instruct"  # 启用新模型
```

2. 重启后端服务
3. 系统自动使用新模型

### 验证当前模型

```python
# 在Python终端中运行
import config
print(config.get_model_info())
```

输出：
```
[当前模型] qwen3-vl-32b-instruct
  描述: Qwen3-VL 32B Instruct版 - 直接指令执行
  类型: local_oss_api
  成本等级: low
  能力: multimodal, streaming, fast_response
```

---

## 📊 评测流程

### 自动评测

每次AI调用都会自动记录到`evaluation_data/evaluation_results.csv`

### 人工评分

1. 打开CSV文件
2. 找到需要评分的记录
3. 填写各项评分（1-5分）
4. 在`notes`列添加观察到的问题

示例CSV内容：
```csv
record_id,model_name,task_type,instruction_following_score,notes
qwen-vl-max_solve_20251030_001,qwen-vl-max,solve,4.5,"OCR识别准确，但公式渲染有小问题"
qwen3-vl-32b-instruct_solve_20251030_002,qwen3-vl-32b-instruct,solve,5.0,"完美执行指令，格式规范"
```

### 生成报告

```bash
cd backend
python evaluation_suite.py
```

或在代码中：
```python
from evaluation_suite import EvaluationLogger, ReportGenerator

logger = EvaluationLogger()
generator = ReportGenerator(logger)
report = generator.generate_report()

print("报告已生成！")
```

报告保存在：`evaluation_reports/evaluation_report_{timestamp}.md`

---

## ⚙️ 本地模型部署配置

### 使用vLLM部署开源模型

#### 1. 安装vLLM
```bash
pip install vllm
```

#### 2. 启动推理服务（32B模型）
```bash
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-VL-32B-Instruct \
    --host 0.0.0.0 \
    --port 8001 \
    --gpu-memory-utilization 0.9
```

#### 3. 启动推理服务（235B模型）
```bash
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-VL-235B-A22B-Instruct \
    --host 0.0.0.0 \
    --port 8002 \
    --tensor-parallel-size 4  # 多GPU并行
```

#### 4. 验证服务
```bash
curl http://localhost:8001/v1/models
```

#### 5. 更新config.py中的API地址
```python
# 确保环境变量设置正确
LOCAL_MODEL_API_BASE=http://localhost:8001/v1
```

---

## 🧪 测试清单

### 功能测试

- [ ] 模型切换：修改`ACTIVE_MODEL_KEY`后重启，验证日志显示正确模型
- [ ] API调用：上传图片，发送解题请求，验证响应正常
- [ ] 流式响应：验证答案逐字显示
- [ ] 错题保存：批改模式下检测到错误，验证自动保存
- [ ] 评测记录：检查`evaluation_data/evaluation_results.csv`是否生成
- [ ] 报告生成：运行评测框架，验证Markdown报告生成

### 性能测试

对每个模型测试：
1. 相同输入下的响应时间
2. 输出质量对比
3. 资源占用（GPU内存、推理速度）

### 对比测试

按照评测框架的维度进行人工评分：
1. 指令遵循
2. 格式正确性
3. OCR准确率
4. 答案正确性
5. 推理质量

---

## 📌 注意事项

### 1. 环境变量

确保`.env`文件包含：
```env
DASHSCOPE_API_KEY=sk-your-key-here
LOCAL_MODEL_API_BASE=http://localhost:8001/v1
```

### 2. 依赖安装

```bash
pip install httpx pandas matplotlib
```

### 3. 兼容性

- 现有API接口保持不变
- 前端无需修改
- 数据库结构不变

### 4. 回滚方案

如果新模型有问题，立即切换回原模型：
```python
ACTIVE_MODEL_KEY = "qwen-vl-max"
```

### 5. 性能优化

- 使用连接池复用HTTP连接
- 本地模型可启用批处理
- 考虑使用异步I/O

---

## 🎯 下一步

1. **完成集成**：按照本指南修改`main_db.py`
2. **部署本地模型**：使用vLLM或类似工具
3. **开始评测**：收集至少每个模型10+样本
4. **人工评分**：填写CSV中的评分字段
5. **生成报告**：运行评测框架
6. **做出决策**：根据报告选择最优模型
7. **生产部署**：切换到选定的模型

---

## 💡 最佳实践

### 评测样本选择

- 简单题目：基础运算、概念题
- 中等题目：多步骤推理
- 复杂题目：综合应用、证明题
- 特殊情况：手写体、模糊图片、复杂公式

### 评分标准

建立统一的评分标准文档，确保不同测试人员的评分一致性。

### 持续监控

生产环境部署后，持续收集用户反馈，定期重新评估。

---

## 📚 参考资料

- **配置文件**: `backend/config.py`
- **适配器**: `backend/model_adapter.py`
- **评测框架**: `backend/evaluation_suite.py`
- **技术报告**: `【工程文档】沐梧AI解题系统完整技术报告V2.md`

---

**集成指南版本**: V1.0  
**最后更新**: 2025-10-30  
**维护者**: 沐梧AI解题系统开发团队

