"""
==============================================================================
æ²æ¢§AIè§£é¢˜ç³»ç»Ÿ - è¯„æµ‹æ¡†æ¶å¿«é€Ÿæµ‹è¯•è„šæœ¬
==============================================================================
åŠŸèƒ½ï¼š
- å¿«é€Ÿæµ‹è¯•æ¨¡å‹åˆ‡æ¢åŠŸèƒ½
- æ¨¡æ‹Ÿè¯„æµ‹æ•°æ®æ”¶é›†
- ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
==============================================================================
"""

import sys
import time
from pathlib import Path

# ç¡®ä¿èƒ½å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))

import config
from model_adapter import get_multimodal_adapter, get_text_adapter
from evaluation_suite import (
    create_evaluation_record,
    EvaluationLogger,
    ReportGenerator,
    quick_evaluate_and_log
)


def print_section(title: str):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70 + "\n")


def test_model_config():
    """æµ‹è¯•æ¨¡å‹é…ç½®"""
    print_section("1. æµ‹è¯•æ¨¡å‹é…ç½®")
    
    # æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
    config.list_all_models()
    
    # æ˜¾ç¤ºå½“å‰æ¨¡å‹
    print("\nå½“å‰æ¿€æ´»çš„æ¨¡å‹:")
    print(config.get_model_info())
    
    # æµ‹è¯•é…ç½®è·å–
    try:
        model_config = config.get_active_model_config()
        print(f"\nâœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡å‹ç±»å‹: {model_config['type']}")
        print(f"   æ¨¡å‹åç§°: {model_config['model_name']}")
        
        if model_config['type'] == 'dashscope_api':
            print(f"   APIå¯†é’¥: {model_config['api_key'][:20]}...")
        else:
            print(f"   APIåœ°å€: {model_config['api_base']}")
        
        return True
    except Exception as e:
        print(f"\nâŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False


def test_model_adapter():
    """æµ‹è¯•æ¨¡å‹é€‚é…å™¨"""
    print_section("2. æµ‹è¯•æ¨¡å‹é€‚é…å™¨")
    
    try:
        adapter = get_multimodal_adapter()
        print(f"âœ… é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ: {adapter.model_name}")
        
        # æ„å»ºç®€å•æµ‹è¯•æ¶ˆæ¯
        test_messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä½æ•°å­¦è€å¸ˆã€‚"
            },
            {
                "role": "user",
                "content": [
                    {"text": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"}
                ]
            }
        ]
        
        print("\nğŸ“¤ å‘é€æµ‹è¯•æ¶ˆæ¯...")
        print("ğŸ’¬ AIå›å¤: ")
        
        start_time = time.time()
        full_response = ""
        
        try:
            for chunk in adapter.call(test_messages, stream=True):
                if chunk["finish_reason"] == "error":
                    print(f"\nâŒ é”™è¯¯: {chunk.get('error')}")
                    return False
                
                content = chunk["content"]
                full_response += content
                print(content, end="", flush=True)
            
            response_time = time.time() - start_time
            
            print(f"\n\nâœ… æµ‹è¯•æˆåŠŸï¼")
            print(f"   å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            print(f"   å›å¤é•¿åº¦: {len(full_response)}å­—ç¬¦")
            
            return True, full_response, response_time
        
        except Exception as e:
            print(f"\nâŒ è°ƒç”¨å¤±è´¥: {e}")
            return False, "", 0
    
    except Exception as e:
        print(f"âŒ é€‚é…å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return False, "", 0


def test_evaluation_logging(response: str, response_time: float):
    """æµ‹è¯•è¯„æµ‹è®°å½•"""
    print_section("3. æµ‹è¯•è¯„æµ‹è®°å½•")
    
    try:
        logger = EvaluationLogger()
        print(f"âœ… è¯„æµ‹è®°å½•å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   CSVæ–‡ä»¶: {logger.csv_path}")
        
        # åˆ›å»ºæµ‹è¯•è®°å½•
        print("\nğŸ“ åˆ›å»ºæµ‹è¯•è¯„æµ‹è®°å½•...")
        
        for task_type in ["solve", "review", "generate"]:
            record = create_evaluation_record(
                model_name=config.ACTIVE_MODEL_KEY,
                task_type=task_type,
                input_prompt=f"æµ‹è¯•{task_type}ä»»åŠ¡çš„è¾“å…¥",
                raw_output=response,
                response_time=response_time,
                token_count=len(response),
                notes=f"è¿™æ˜¯{task_type}ä»»åŠ¡çš„æµ‹è¯•è®°å½•",
                typical_failures=["æµ‹è¯•å¤±è´¥æ¡ˆä¾‹1", "æµ‹è¯•å¤±è´¥æ¡ˆä¾‹2"]
            )
            
            logger.log_evaluation(record)
            print(f"   âœ“ {task_type}ä»»åŠ¡è®°å½•å·²ä¿å­˜")
        
        # åŠ è½½è®°å½•éªŒè¯
        all_records = logger.load_all_records()
        print(f"\nâœ… è¯„æµ‹è®°å½•æµ‹è¯•å®Œæˆ")
        print(f"   å½“å‰æ€»è®°å½•æ•°: {len(all_records)}")
        
        return True
    
    except Exception as e:
        print(f"âŒ è¯„æµ‹è®°å½•æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_report_generation():
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ"""
    print_section("4. æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ")
    
    try:
        logger = EvaluationLogger()
        generator = ReportGenerator(logger)
        
        print("ğŸ“Š ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š...")
        report = generator.generate_report()
        
        print(f"\nâœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼")
        print(f"   æŠ¥å‘Šé•¿åº¦: {len(report)}å­—ç¬¦")
        
        # æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆ
        print("\nğŸ“„ æŠ¥å‘Šé¢„è§ˆï¼ˆå‰800å­—ç¬¦ï¼‰:")
        print("-" * 70)
        print(report[:800])
        print("...")
        print("-" * 70)
        
        return True
    
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return False


def simulate_batch_evaluation():
    """æ¨¡æ‹Ÿæ‰¹é‡è¯„æµ‹"""
    print_section("5. æ¨¡æ‹Ÿæ‰¹é‡è¯„æµ‹ï¼ˆå¯é€‰ï¼‰")
    
    response = input("\næ˜¯å¦è¦æ¨¡æ‹Ÿç”Ÿæˆæ›´å¤šè¯„æµ‹æ•°æ®ï¼Ÿ(y/n): ").strip().lower()
    
    if response != 'y':
        print("è·³è¿‡æ‰¹é‡è¯„æµ‹æ¨¡æ‹Ÿ")
        return
    
    print("\nå¼€å§‹æ¨¡æ‹Ÿæ‰¹é‡è¯„æµ‹...")
    
    # æ¨¡æ‹Ÿæ•°æ®
    test_cases = [
        {
            "task_type": "solve",
            "prompt": "è§£è¿™é“ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹ï¼šxÂ² + 5x + 6 = 0",
            "output": "è§£ï¼šxÂ² + 5x + 6 = 0\nä½¿ç”¨å› å¼åˆ†è§£æ³•ï¼š\n(x + 2)(x + 3) = 0\nå› æ­¤ xâ‚ = -2, xâ‚‚ = -3"
        },
        {
            "task_type": "review",
            "prompt": "æ‰¹æ”¹è¿™é“é¢˜çš„ç­”æ¡ˆ",
            "output": "æ‚¨çš„ç­”æ¡ˆä¸­æœ‰ä»¥ä¸‹é”™è¯¯ï¼š\n1. ç¬¦å·é”™è¯¯ï¼šç¬¬äºŒæ­¥åº”è¯¥æ˜¯+è€Œä¸æ˜¯-\n2. è®¡ç®—é”™è¯¯ï¼šæœ€åçš„ç»“æœåº”è¯¥æ˜¯15è€Œä¸æ˜¯13"
        },
        {
            "task_type": "generate",
            "prompt": "ç”Ÿæˆ3é“å…³äºä¸‰è§’å‡½æ•°çš„ç»ƒä¹ é¢˜",
            "output": "## é¢˜ç›®1\næ±‚sin(Ï€/6)çš„å€¼\n\n**ç­”æ¡ˆï¼š** 1/2\n\n**è§£æï¼š** ...\n\n---"
        }
    ]
    
    logger = EvaluationLogger()
    
    for i, case in enumerate(test_cases, 1):
        print(f"\nç”Ÿæˆæ ·æœ¬ {i}/{len(test_cases)}...")
        
        quick_evaluate_and_log(
            model_name=config.ACTIVE_MODEL_KEY,
            task_type=case["task_type"],
            input_prompt=case["prompt"],
            raw_output=case["output"],
            response_time=2.5,
            token_count=len(case["output"]),
            notes=f"æ¨¡æ‹Ÿæ ·æœ¬{i}"
        )
    
    print(f"\nâœ… å·²ç”Ÿæˆ {len(test_cases)} ä¸ªæ¨¡æ‹Ÿè¯„æµ‹æ ·æœ¬")


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "=" * 70)
    print("  æ²æ¢§AIè§£é¢˜ç³»ç»Ÿ - è¯„æµ‹æ¡†æ¶å¿«é€Ÿæµ‹è¯•")
    print("=" * 70)
    
    # æµ‹è¯•1: æ¨¡å‹é…ç½®
    if not test_model_config():
        print("\nâš ï¸  æ¨¡å‹é…ç½®æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥config.pyå’Œç¯å¢ƒå˜é‡")
        return
    
    # æµ‹è¯•2: æ¨¡å‹é€‚é…å™¨
    result = test_model_adapter()
    if isinstance(result, tuple) and result[0]:
        _, response, response_time = result
    else:
        print("\nâš ï¸  æ¨¡å‹é€‚é…å™¨æµ‹è¯•å¤±è´¥")
        print("   å¦‚æœæ˜¯æœ¬åœ°æ¨¡å‹ï¼Œè¯·ç¡®ä¿æ¨ç†æœåŠ¡å·²å¯åŠ¨")
        print("   å¦‚æœæ˜¯Dashscope APIï¼Œè¯·æ£€æŸ¥APIå¯†é’¥")
        
        # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ç»§ç»­æµ‹è¯•
        print("\n   ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ç»§ç»­æµ‹è¯•è¯„æµ‹åŠŸèƒ½...")
        response = "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„AIå“åº”ï¼Œç”¨äºæµ‹è¯•è¯„æµ‹æ¡†æ¶ã€‚"
        response_time = 2.5
    
    # æµ‹è¯•3: è¯„æµ‹è®°å½•
    if not test_evaluation_logging(response, response_time):
        print("\nâš ï¸  è¯„æµ‹è®°å½•æµ‹è¯•å¤±è´¥")
        return
    
    # æµ‹è¯•4: æŠ¥å‘Šç”Ÿæˆ
    if not test_report_generation():
        print("\nâš ï¸  æŠ¥å‘Šç”Ÿæˆæµ‹è¯•å¤±è´¥")
        return
    
    # æµ‹è¯•5: æ‰¹é‡è¯„æµ‹ï¼ˆå¯é€‰ï¼‰
    simulate_batch_evaluation()
    
    # æ€»ç»“
    print_section("âœ… æµ‹è¯•å®Œæˆ")
    
    print("æµ‹è¯•ç»“æœ:")
    print("  âœ“ æ¨¡å‹é…ç½®æ­£å¸¸")
    print("  âœ“ æ¨¡å‹é€‚é…å™¨æ­£å¸¸")
    print("  âœ“ è¯„æµ‹è®°å½•æ­£å¸¸")
    print("  âœ“ æŠ¥å‘Šç”Ÿæˆæ­£å¸¸")
    
    print("\nä¸‹ä¸€æ­¥:")
    print("  1. æŸ¥çœ‹è¯„æµ‹æ•°æ®: evaluation_data/evaluation_results.csv")
    print("  2. æŸ¥çœ‹è¯„æµ‹æŠ¥å‘Š: evaluation_reports/evaluation_report_*.md")
    print("  3. åˆ‡æ¢æ¨¡å‹æµ‹è¯•: ä¿®æ”¹config.pyä¸­çš„ACTIVE_MODEL_KEY")
    print("  4. é›†æˆåˆ°main_db.py: å‚è€ƒã€é›†æˆæŒ‡å—ã€‘å¤§æ¨¡å‹åˆ‡æ¢ä¸è¯„æµ‹æ¡†æ¶.md")
    
    print("\n" + "=" * 70)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

