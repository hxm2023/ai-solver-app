# å¤§æ¨¡å‹æ›¿æ¢ä¸è¯„æµ‹æ¡†æ¶ - README

## ğŸ¯ é¡¹ç›®ç®€ä»‹

ä¸º"æ²æ¢§AIè§£é¢˜ç³»ç»Ÿ"å¼€å‘çš„**å¤§æ¨¡å‹åˆ‡æ¢æœºåˆ¶**å’Œ**å®Œæ•´è¯„æµ‹æ¡†æ¶**ï¼Œæ”¯æŒä»é—­æºæ¨¡å‹ï¼ˆqwen-vl-maxï¼‰å‘å¼€æºæ¨¡å‹çš„å¹³æ»‘è¿ç§»å’Œé‡åŒ–è¯„ä¼°ã€‚

---

## ğŸ“¦ æ ¸å¿ƒæ¨¡å—

### 1. config.py - æ¨¡å‹é…ç½®ä¸­å¿ƒ
**ä¸€é”®åˆ‡æ¢æ¨¡å‹ï¼Œæ— éœ€ä¿®æ”¹ä¸šåŠ¡ä»£ç **

```python
# åªéœ€ä¿®æ”¹è¿™ä¸€è¡Œ
ACTIVE_MODEL_KEY = "qwen3-vl-32b-instruct"
```

**æ”¯æŒ5ç§æ¨¡å‹ï¼š**
- `qwen-vl-max` - é—­æºåŸºå‡†
- `qwen3-vl-32b-thinking` - å¼€æºï¼Œæ€è€ƒé“¾
- `qwen3-vl-32b-instruct` - å¼€æºï¼Œç›´æ¥æŒ‡ä»¤ï¼ˆæ¨èï¼‰
- `qwen3-vl-235b-a22b-thinking` - é«˜æ€§èƒ½ï¼Œæ€è€ƒé“¾
- `qwen3-vl-235b-a22b-instruct` - é«˜æ€§èƒ½ï¼Œç›´æ¥æŒ‡ä»¤

### 2. model_adapter.py - ç»Ÿä¸€é€‚é…å™¨
**è‡ªåŠ¨é€‚é…ä¸åŒAPIæ ¼å¼ï¼Œä¿æŒæ¥å£ä¸€è‡´**

```python
from model_adapter import get_multimodal_adapter

adapter = get_multimodal_adapter()  # è‡ªåŠ¨ä½¿ç”¨é…ç½®çš„æ¨¡å‹
for chunk in adapter.call(messages, stream=True):
    print(chunk["content"], end="")
```

### 3. evaluation_suite.py - è¯„æµ‹æ¡†æ¶
**è‡ªåŠ¨è®°å½•è¯„æµ‹æ•°æ®ï¼Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š**

```python
from evaluation_suite import quick_evaluate_and_log

quick_evaluate_and_log(
    model_name="qwen3-vl-32b-instruct",
    task_type="solve",
    input_prompt="è§£è¿™é“é¢˜...",
    raw_output="è§£ç­”ï¼š...",
    notes="OCRå‡†ç¡®"
)
```

---

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### Step 1: æµ‹è¯•æ¡†æ¶
```bash
cd backend
python run_evaluation_test.py
```

### Step 2: åˆ‡æ¢æ¨¡å‹
ç¼–è¾‘ `config.py`:
```python
ACTIVE_MODEL_KEY = "qwen3-vl-32b-instruct"
```

### Step 3: é›†æˆåˆ°é¡¹ç›®
å‚è€ƒ `ã€é›†æˆæŒ‡å—ã€‘å¤§æ¨¡å‹åˆ‡æ¢ä¸è¯„æµ‹æ¡†æ¶.md`

æ ¸å¿ƒä¿®æ”¹ï¼š
```python
# 1. å¯¼å…¥
from model_adapter import get_multimodal_adapter
from evaluation_suite import quick_evaluate_and_log

# 2. ä½¿ç”¨é€‚é…å™¨
adapter = get_multimodal_adapter()
response = adapter.call(messages)

# 3. è®°å½•è¯„æµ‹
quick_evaluate_and_log(model_name, task_type, prompt, output)
```

---

## ğŸ“Š è¯„æµ‹æµç¨‹

### 1. æ”¶é›†æ•°æ®ï¼ˆå»ºè®®æ¯æ¨¡å‹20+æ ·æœ¬ï¼‰

è‡ªåŠ¨è®°å½•åˆ° `evaluation_data/evaluation_results.csv`

### 2. äººå·¥è¯„åˆ†ï¼ˆ1-5åˆ†åˆ¶ï¼‰

åœ¨CSVä¸­å¡«å†™13ä¸ªç»´åº¦çš„è¯„åˆ†ï¼š
- é€šç”¨ï¼šæŒ‡ä»¤éµå¾ªã€æ ¼å¼æ­£ç¡®ã€æ— å¹»è§‰
- è§£é¢˜ï¼šOCRå‡†ç¡®ã€ç­”æ¡ˆæ­£ç¡®ã€æ¨ç†è´¨é‡
- æ‰¹æ”¹ï¼šé”™è¯¯æ£€æµ‹ã€è§£ææ¸…æ™°ã€çŸ¥è¯†ç‚¹å‡†ç¡®
- ç”Ÿé¢˜ï¼šé¢˜ç›®ç›¸å…³ã€åˆ›æ–°éš¾åº¦ã€ç­”æ¡ˆå®Œæ•´

### 3. ç”ŸæˆæŠ¥å‘Š

```python
from evaluation_suite import EvaluationLogger, ReportGenerator

generator = ReportGenerator(EvaluationLogger())
generator.generate_report()
```

æŠ¥å‘Šä¿å­˜åœ¨ `evaluation_reports/evaluation_report_{timestamp}.md`

---

## ğŸ“ æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | è¯´æ˜ | è¡Œæ•° |
|------|------|------|
| `config.py` | æ¨¡å‹é…ç½®ä¸­å¿ƒ | 254 |
| `model_adapter.py` | ç»Ÿä¸€é€‚é…å™¨ | 457 |
| `evaluation_suite.py` | è¯„æµ‹æ¡†æ¶ | 852 |
| `run_evaluation_test.py` | å¿«é€Ÿæµ‹è¯•è„šæœ¬ | 246 |
| `ã€é›†æˆæŒ‡å—ã€‘å¤§æ¨¡å‹åˆ‡æ¢ä¸è¯„æµ‹æ¡†æ¶.md` | è¯¦ç»†é›†æˆæŒ‡å— | - |
| `ã€å®Œæˆã€‘å¤§æ¨¡å‹æ›¿æ¢ä¸è¯„æµ‹æ¡†æ¶å¼€å‘æŠ¥å‘Š.md` | å¼€å‘æŠ¥å‘Š | - |
| `README_MODEL_EVALUATION.md` | æœ¬æ–‡æ¡£ | - |

---

## ğŸ’¡ è®¾è®¡äº®ç‚¹

âœ… **é›¶ä¾µå…¥**ï¼šå¯¹ç°æœ‰ä»£ç æ— ä¾µå…¥  
âœ… **ä¸€é”®åˆ‡æ¢**ï¼šä¿®æ”¹1è¡Œä»£ç å³å¯  
âœ… **ç»Ÿä¸€æ¥å£**ï¼šæ— éœ€å…³å¿ƒæ¨¡å‹å·®å¼‚  
âœ… **è‡ªåŠ¨è¯„æµ‹**ï¼šä¸å½±å“ä¸šåŠ¡æ€§èƒ½  
âœ… **å®šæ€§åˆ†æ**ï¼šè®°å½•å…¸å‹é—®é¢˜ï¼ŒæŒ‡å¯¼å¾®è°ƒ  
âœ… **è‡ªåŠ¨æŠ¥å‘Š**ï¼šæ•°æ®é©±åŠ¨å†³ç­–  

---

## ğŸ¯ è¯„æµ‹ç»´åº¦ï¼ˆ13ä¸ªï¼‰

### é€šç”¨ç»´åº¦ï¼ˆ3ä¸ªï¼‰
1. instruction_following_score - æŒ‡ä»¤éµå¾ª
2. format_correction_score - æ ¼å¼æ­£ç¡®æ€§
3. hallucination_score - å¹»è§‰æ£€æµ‹

### è§£é¢˜ä»»åŠ¡ï¼ˆ3ä¸ªï¼‰
4. ocr_accuracy_score - OCRå‡†ç¡®ç‡
5. correctness_score - ç­”æ¡ˆæ­£ç¡®æ€§
6. reasoning_quality_score - é€»è¾‘æ¨ç†è´¨é‡

### æ‰¹æ”¹ä»»åŠ¡ï¼ˆ3ä¸ªï¼‰
7. error_detection_score - é”™è¯¯æ£€æµ‹èƒ½åŠ›
8. explanation_clarity_score - è§£ææ¸…æ™°åº¦
9. knowledge_point_accuracy_score - çŸ¥è¯†ç‚¹å‡†ç¡®æ€§

### ç”Ÿé¢˜ä»»åŠ¡ï¼ˆ3ä¸ªï¼‰
10. relevance_score - é¢˜ç›®ç›¸å…³æ€§
11. creativity_difficulty_score - åˆ›æ–°ä¸éš¾åº¦
12. answer_integrity_score - ç­”æ¡ˆå®Œæ•´æ€§

### æ€§èƒ½æŒ‡æ ‡ï¼ˆ2ä¸ªï¼‰
13. response_time_seconds - å“åº”æ—¶é—´
14. token_count - Tokenæ•°é‡

---

## ğŸ”§ æœ¬åœ°æ¨¡å‹éƒ¨ç½²

ä½¿ç”¨vLLMï¼š

```bash
# 32Bæ¨¡å‹ï¼ˆå•GPUï¼‰
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-VL-32B-Instruct \
    --host 0.0.0.0 \
    --port 8001

# 235Bæ¨¡å‹ï¼ˆå¤šGPUï¼‰
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-VL-235B-A22B-Instruct \
    --host 0.0.0.0 \
    --port 8002 \
    --tensor-parallel-size 4
```

---

## ğŸ“ˆ é¢„æœŸè¯„æµ‹ç»“æœ

åŸºäºQwen3-VLç‰¹ç‚¹ï¼š

| æ¨¡å‹ | æ€§ä»·æ¯” | æŒ‡ä»¤éµå¾ª | æ¨ç†èƒ½åŠ› | OCRèƒ½åŠ› | æ¨èåœºæ™¯ |
|------|--------|----------|----------|---------|----------|
| 32B-Instruct | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | ç”Ÿäº§é¦–é€‰ |
| 32B-Thinking | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | å¤æ‚æ¨ç† |
| 235B-Instruct | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | é«˜è´¨é‡è¦æ±‚ |
| 235B-Thinking | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | æœ€é«˜æ€§èƒ½ |

**æ¨èç­–ç•¥ï¼š**
- å¼€å‘/æµ‹è¯• â†’ 32B-Instruct
- ç”Ÿäº§ï¼ˆé«˜å¹¶å‘ï¼‰ â†’ 32B-Instruct
- ç”Ÿäº§ï¼ˆé«˜è´¨é‡ï¼‰ â†’ 235B-A22B-Thinking

---

## ğŸ› ï¸ ä¾èµ–å®‰è£…

```bash
pip install httpx pandas matplotlib
```

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåˆ‡æ¢æ¨¡å‹å¹¶æµ‹è¯•

```bash
# 1. åˆ‡æ¢åˆ°32B-Instruct
# ç¼–è¾‘ config.py: ACTIVE_MODEL_KEY = "qwen3-vl-32b-instruct"

# 2. æµ‹è¯•
python run_evaluation_test.py

# 3. æŸ¥çœ‹ç»“æœ
cat evaluation_data/evaluation_results.csv
```

### ç¤ºä¾‹2ï¼šåœ¨ä¸šåŠ¡ä»£ç ä¸­ä½¿ç”¨

```python
# åŸä»£ç 
response = dashscope.MultiModalConversation.call(
    model='qwen-vl-max',
    messages=messages
)

# æ–°ä»£ç ï¼ˆä½¿ç”¨é€‚é…å™¨ï¼‰
from model_adapter import get_multimodal_adapter

adapter = get_multimodal_adapter()  # è‡ªåŠ¨ä½¿ç”¨config.pyä¸­çš„æ¨¡å‹
for chunk in adapter.call(messages, stream=True):
    # ä¸šåŠ¡é€»è¾‘ä¸å˜
    yield chunk["content"]
```

### ç¤ºä¾‹3ï¼šè®°å½•è¯„æµ‹å¹¶ç”ŸæˆæŠ¥å‘Š

```python
from evaluation_suite import (
    quick_evaluate_and_log,
    EvaluationLogger,
    ReportGenerator
)

# åœ¨AIå“åº”å®Œæˆåè®°å½•
quick_evaluate_and_log(
    model_name=config.ACTIVE_MODEL_KEY,
    task_type="solve",
    input_prompt=user_input,
    raw_output=ai_output,
    response_time=2.5,
    notes="æ‰‹å†™ä½“è¯†åˆ«å‡†ç¡®"
)

# æ”¶é›†è¶³å¤Ÿæ ·æœ¬åç”ŸæˆæŠ¥å‘Š
generator = ReportGenerator(EvaluationLogger())
report = generator.generate_report()
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ç¯å¢ƒå˜é‡**ï¼šç¡®ä¿`.env`åŒ…å«`DASHSCOPE_API_KEY`
2. **æœ¬åœ°æ¨¡å‹**ï¼šéœ€è¦å…ˆå¯åŠ¨æ¨ç†æœåŠ¡
3. **GPUèµ„æº**ï¼š235Bæ¨¡å‹éœ€è¦å¤šGPU
4. **APIå…¼å®¹**ï¼šæœ¬åœ°æ¨¡å‹éœ€è¦OpenAIå…¼å®¹æ¥å£

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **é›†æˆæŒ‡å—**ï¼š`ã€é›†æˆæŒ‡å—ã€‘å¤§æ¨¡å‹åˆ‡æ¢ä¸è¯„æµ‹æ¡†æ¶.md`
- **å¼€å‘æŠ¥å‘Š**ï¼š`ã€å®Œæˆã€‘å¤§æ¨¡å‹æ›¿æ¢ä¸è¯„æµ‹æ¡†æ¶å¼€å‘æŠ¥å‘Š.md`
- **æŠ€æœ¯æŠ¥å‘Š**ï¼š`../ã€å·¥ç¨‹æ–‡æ¡£ã€‘æ²æ¢§AIè§£é¢˜ç³»ç»Ÿå®Œæ•´æŠ€æœ¯æŠ¥å‘ŠV2.md`

---

## âœ… æµ‹è¯•æ¸…å•

- [ ] config.pyåŠ è½½æ­£å¸¸
- [ ] æ¨¡å‹åˆ‡æ¢æˆåŠŸ
- [ ] é€‚é…å™¨è°ƒç”¨æ­£å¸¸
- [ ] è¯„æµ‹è®°å½•æ­£å¸¸
- [ ] æŠ¥å‘Šç”ŸæˆæˆåŠŸ
- [ ] æœ¬åœ°æ¨¡å‹è¿æ¥æˆåŠŸï¼ˆå¦‚ä½¿ç”¨ï¼‰

---

## ğŸ“ é—®é¢˜æ’æŸ¥

### Q1: æ¨¡å‹åˆ‡æ¢ä¸ç”Ÿæ•ˆï¼Ÿ
**A**: é‡å¯Pythonè¿›ç¨‹/åç«¯æœåŠ¡

### Q2: æœ¬åœ°æ¨¡å‹è¿æ¥å¤±è´¥ï¼Ÿ
**A**: 
1. æ£€æŸ¥æ¨ç†æœåŠ¡æ˜¯å¦å¯åŠ¨
2. éªŒè¯APIåœ°å€ï¼š`curl http://localhost:8001/v1/models`
3. æ£€æŸ¥é˜²ç«å¢™

### Q3: è¯„æµ‹æ•°æ®ä¸ä¿å­˜ï¼Ÿ
**A**: æ£€æŸ¥`evaluation_data/`ç›®å½•æƒé™

### Q4: æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Ÿ
**A**: 
1. ç¡®ä¿è‡³å°‘æœ‰1æ¡è¯„æµ‹è®°å½•
2. æ£€æŸ¥pandasæ˜¯å¦å®‰è£…ï¼š`pip install pandas`

---

## ğŸ‰ å¿«é€Ÿæµ‹è¯•

```bash
# ä¸€é”®æµ‹è¯•æ‰€æœ‰åŠŸèƒ½
cd backend
python run_evaluation_test.py

# é¢„æœŸè¾“å‡ºï¼š
# âœ… æ¨¡å‹é…ç½®æ­£å¸¸
# âœ… æ¨¡å‹é€‚é…å™¨æ­£å¸¸
# âœ… è¯„æµ‹è®°å½•æ­£å¸¸
# âœ… æŠ¥å‘Šç”Ÿæˆæ­£å¸¸
```

---

**å¼€å‘å®Œæˆ**: 2025-10-30  
**çŠ¶æ€**: âœ… å·²å®Œæˆï¼Œå¾…é›†æˆ  
**ç»´æŠ¤**: æ²æ¢§AIè§£é¢˜ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ

**ç¥è¯„æµ‹é¡ºåˆ©ï¼** ğŸš€

