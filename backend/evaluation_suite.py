"""
==============================================================================
æ²æ¢§AIè§£é¢˜ç³»ç»Ÿ - æ¨¡å‹èƒ½åŠ›è¯„ä¼°æ¡†æ¶
==============================================================================
åŠŸèƒ½ï¼š
- è®°å½•æ¯æ¬¡AIäº¤äº’çš„è¯„æµ‹æ•°æ®
- å¤šç»´åº¦è¯„åˆ†ç³»ç»Ÿ
- è‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š
==============================================================================
"""

import os
import csv
import json
import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional, Literal
from dataclasses import dataclass, asdict, field
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

# ==============================================================================
# é…ç½®
# ==============================================================================

# è¯„æµ‹æ•°æ®å­˜å‚¨ç›®å½•
EVALUATION_DATA_DIR = Path("evaluation_data")
EVALUATION_DATA_DIR.mkdir(exist_ok=True)

# CSVæ–‡ä»¶è·¯å¾„
EVALUATION_CSV = EVALUATION_DATA_DIR / "evaluation_results.csv"

# æŠ¥å‘Šè¾“å‡ºç›®å½•
REPORTS_DIR = Path("evaluation_reports")
REPORTS_DIR.mkdir(exist_ok=True)

# ==============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ==============================================================================

TaskType = Literal["solve", "review", "generate"]

@dataclass
class EvaluationRecord:
    """å•æ¬¡è¯„æµ‹è®°å½•"""
    
    # åŸºæœ¬ä¿¡æ¯
    record_id: str
    model_name: str
    task_type: TaskType
    timestamp: str
    
    # è¾“å…¥è¾“å‡º
    input_prompt: str
    input_image_path: Optional[str]
    raw_output: str
    
    # é€šç”¨è¯„åˆ† (1-5åˆ†)
    instruction_following_score: float = 0.0  # æŒ‡ä»¤éµå¾ª
    format_correction_score: float = 0.0      # æ ¼å¼æ­£ç¡®æ€§
    hallucination_score: float = 0.0          # å¹»è§‰æ£€æµ‹ï¼ˆ5åˆ†=æ— å¹»è§‰ï¼‰
    
    # è§£é¢˜ä»»åŠ¡è¯„åˆ†
    ocr_accuracy_score: float = 0.0           # OCRå‡†ç¡®ç‡
    correctness_score: float = 0.0            # ç­”æ¡ˆæ­£ç¡®æ€§
    reasoning_quality_score: float = 0.0      # é€»è¾‘æ¨ç†è´¨é‡
    
    # æ‰¹æ”¹ä»»åŠ¡è¯„åˆ†
    error_detection_score: float = 0.0        # é”™è¯¯æ£€æµ‹èƒ½åŠ›
    explanation_clarity_score: float = 0.0    # è§£ææ¸…æ™°åº¦
    knowledge_point_accuracy_score: float = 0.0  # çŸ¥è¯†ç‚¹æå–å‡†ç¡®æ€§
    
    # ç”Ÿé¢˜ä»»åŠ¡è¯„åˆ†
    relevance_score: float = 0.0              # é¢˜ç›®ç›¸å…³æ€§
    creativity_difficulty_score: float = 0.0  # åˆ›æ–°ä¸éš¾åº¦
    answer_integrity_score: float = 0.0       # ç­”æ¡ˆå®Œæ•´æ€§
    
    # å®šæ€§è®°å½•
    notes: str = ""                           # æµ‹è¯•äººå‘˜å¤‡æ³¨
    typical_failures: List[str] = field(default_factory=list)  # å…¸å‹å¤±è´¥æ¡ˆä¾‹
    
    # å…ƒæ•°æ®
    response_time_seconds: float = 0.0        # å“åº”æ—¶é—´
    token_count: int = 0                      # Tokenæ•°é‡
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        data = asdict(self)
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ä»¥ä¾¿å­˜å‚¨
        data['typical_failures'] = json.dumps(data['typical_failures'], ensure_ascii=False)
        return data
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'EvaluationRecord':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        if isinstance(data['typical_failures'], str):
            data['typical_failures'] = json.loads(data['typical_failures'])
        return cls(**data)


# ==============================================================================
# è¯„åˆ†ç³»ç»Ÿ
# ==============================================================================

class EvaluationScorer:
    """è¯„åˆ†ç³»ç»Ÿï¼ˆäººå·¥è¾…åŠ©ï¼‰"""
    
    @staticmethod
    def score_task(
        model_output: str,
        task_type: TaskType,
        ground_truth: Optional[Dict] = None,
        manual_scores: Optional[Dict[str, float]] = None
    ) -> Dict[str, float]:
        """
        å¯¹ä»»åŠ¡è¿›è¡Œè¯„åˆ†
        
        Args:
            model_output: æ¨¡å‹è¾“å‡º
            task_type: ä»»åŠ¡ç±»å‹
            ground_truth: çœŸå€¼æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            manual_scores: äººå·¥è¯„åˆ†ï¼ˆå¦‚æœå·²è¯„ï¼‰
        
        Returns:
            Dict[str, float]: å„ç»´åº¦è¯„åˆ†
        """
        scores = {}
        
        # å¦‚æœæä¾›äº†äººå·¥è¯„åˆ†ï¼Œç›´æ¥ä½¿ç”¨
        if manual_scores:
            return manual_scores
        
        # å¦åˆ™è¿›è¡Œè‡ªåŠ¨è¯„åˆ†ï¼ˆåŸºäºè§„åˆ™ï¼Œåç»­å¯äººå·¥è°ƒæ•´ï¼‰
        
        # é€šç”¨ç»´åº¦è‡ªåŠ¨è¯„åˆ†
        scores['instruction_following_score'] = EvaluationScorer._auto_score_instruction_following(
            model_output, task_type
        )
        scores['format_correction_score'] = EvaluationScorer._auto_score_format(model_output)
        scores['hallucination_score'] = EvaluationScorer._auto_score_hallucination(model_output)
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è¯„åˆ†
        if task_type == "solve":
            scores['ocr_accuracy_score'] = 0.0  # éœ€è¦äººå·¥è¯„åˆ†
            scores['correctness_score'] = 0.0   # éœ€è¦äººå·¥è¯„åˆ†
            scores['reasoning_quality_score'] = EvaluationScorer._auto_score_reasoning(model_output)
        
        elif task_type == "review":
            scores['error_detection_score'] = 0.0  # éœ€è¦äººå·¥è¯„åˆ†
            scores['explanation_clarity_score'] = EvaluationScorer._auto_score_clarity(model_output)
            scores['knowledge_point_accuracy_score'] = 0.0  # éœ€è¦äººå·¥è¯„åˆ†
        
        elif task_type == "generate":
            scores['relevance_score'] = 0.0  # éœ€è¦äººå·¥è¯„åˆ†
            scores['creativity_difficulty_score'] = 0.0  # éœ€è¦äººå·¥è¯„åˆ†
            scores['answer_integrity_score'] = EvaluationScorer._auto_score_answer_integrity(model_output)
        
        return scores
    
    @staticmethod
    def _auto_score_instruction_following(output: str, task_type: TaskType) -> float:
        """è‡ªåŠ¨è¯„åˆ†ï¼šæŒ‡ä»¤éµå¾ª"""
        score = 5.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾è¿åæŒ‡ä»¤çš„æƒ…å†µ
        if "æŠ±æ­‰" in output and "æ— æ³•" in output:
            score -= 1.0
        
        # æ£€æŸ¥æ˜¯å¦æŒ‰ç…§ä»»åŠ¡ç±»å‹æ­£ç¡®è¾“å‡º
        if task_type == "solve" and "è§£ç­”" not in output and "è§£æ" not in output:
            score -= 0.5
        
        return max(1.0, score)
    
    @staticmethod
    def _auto_score_format(output: str) -> float:
        """è‡ªåŠ¨è¯„åˆ†ï¼šæ ¼å¼æ­£ç¡®æ€§"""
        score = 5.0
        
        # æ£€æŸ¥LaTeXå…¬å¼æ ¼å¼
        if "$" in output or "\\(" in output:
            # æ£€æŸ¥é…å¯¹
            dollar_count = output.count("$")
            if dollar_count % 2 != 0:
                score -= 1.0  # LaTeXæœªé…å¯¹
        
        # æ£€æŸ¥Markdownæ ¼å¼
        if "##" in output or "**" in output:
            score += 0.5  # ä½¿ç”¨äº†Markdownæ ¼å¼
        
        return min(5.0, max(1.0, score))
    
    @staticmethod
    def _auto_score_hallucination(output: str) -> float:
        """è‡ªåŠ¨è¯„åˆ†ï¼šå¹»è§‰æ£€æµ‹ï¼ˆ5åˆ†=æ— å¹»è§‰ï¼‰"""
        score = 5.0
        
        # æ£€æŸ¥å¸¸è§çš„å¹»è§‰ç‰¹å¾
        hallucination_keywords = [
            "æ ¹æ®æˆ‘çœ‹åˆ°çš„å›¾ç‰‡", "å›¾ç‰‡ä¸­æ˜¾ç¤º", "ä»å›¾ç‰‡å¯ä»¥çœ‹å‡º"
        ]
        
        # å¦‚æœè¾“å‡ºè¿‡çŸ­ï¼Œå¯èƒ½æ˜¯æœ‰é—®é¢˜çš„
        if len(output) < 50:
            score -= 1.0
        
        return max(1.0, score)
    
    @staticmethod
    def _auto_score_reasoning(output: str) -> float:
        """è‡ªåŠ¨è¯„åˆ†ï¼šé€»è¾‘æ¨ç†è´¨é‡"""
        score = 3.0  # åŸºç¡€åˆ†
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ­¥éª¤æ ‡è®°
        step_markers = ["æ­¥éª¤", "ç¬¬ä¸€", "ç¬¬äºŒ", "é¦–å…ˆ", "ç„¶å", "æœ€å", "å› æ­¤"]
        step_count = sum(1 for marker in step_markers if marker in output)
        score += min(step_count * 0.3, 2.0)
        
        # æ£€æŸ¥é•¿åº¦ï¼ˆæ›´è¯¦ç»†é€šå¸¸æ›´å¥½ï¼‰
        if len(output) > 500:
            score += 0.5
        
        return min(5.0, max(1.0, score))
    
    @staticmethod
    def _auto_score_clarity(output: str) -> float:
        """è‡ªåŠ¨è¯„åˆ†ï¼šæ¸…æ™°åº¦"""
        score = 3.0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æ„åŒ–è¾“å‡º
        if "**" in output or "##" in output:
            score += 1.0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é€»è¾‘è¿æ¥è¯
        connectors = ["å› ä¸º", "æ‰€ä»¥", "ä½†æ˜¯", "ç„¶è€Œ", "å› æ­¤", "ç”±äº"]
        if any(conn in output for conn in connectors):
            score += 0.5
        
        return min(5.0, max(1.0, score))
    
    @staticmethod
    def _auto_score_answer_integrity(output: str) -> float:
        """è‡ªåŠ¨è¯„åˆ†ï¼šç­”æ¡ˆå®Œæ•´æ€§"""
        score = 3.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç­”æ¡ˆæ ‡è®°
        if "ç­”æ¡ˆ" in output or "è§£æ" in output:
            score += 1.0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯¦ç»†è§£é‡Š
        if len(output) > 300:
            score += 1.0
        
        return min(5.0, max(1.0, score))


# ==============================================================================
# æ•°æ®è®°å½•
# ==============================================================================

class EvaluationLogger:
    """è¯„æµ‹æ•°æ®è®°å½•å™¨"""
    
    def __init__(self, csv_path: Path = EVALUATION_CSV):
        self.csv_path = csv_path
        self._init_csv()
    
    def _init_csv(self):
        """åˆå§‹åŒ–CSVæ–‡ä»¶"""
        if not self.csv_path.exists():
            # åˆ›å»ºCSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
            record = EvaluationRecord(
                record_id="",
                model_name="",
                task_type="solve",
                timestamp="",
                input_prompt="",
                input_image_path=None,
                raw_output=""
            )
            
            with open(self.csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=record.to_dict().keys())
                writer.writeheader()
            
            print(f"âœ… åˆå§‹åŒ–è¯„æµ‹CSVæ–‡ä»¶: {self.csv_path}")
    
    def log_evaluation(self, record: EvaluationRecord) -> None:
        """
        è®°å½•ä¸€æ¬¡è¯„æµ‹
        
        Args:
            record: è¯„æµ‹è®°å½•
        """
        with open(self.csv_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=record.to_dict().keys())
            writer.writerow(record.to_dict())
        
        print(f"âœ… [è¯„æµ‹è®°å½•] {record.model_name} - {record.task_type} - {record.record_id}")
    
    def load_all_records(self) -> List[EvaluationRecord]:
        """åŠ è½½æ‰€æœ‰è¯„æµ‹è®°å½•"""
        records = []
        
        if not self.csv_path.exists():
            return records
        
        with open(self.csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # è½¬æ¢æ•°å€¼å­—æ®µ
                for key in row:
                    if 'score' in key or key == 'response_time_seconds':
                        row[key] = float(row[key]) if row[key] else 0.0
                    elif key == 'token_count':
                        row[key] = int(row[key]) if row[key] else 0
                
                records.append(EvaluationRecord.from_dict(row))
        
        return records
    
    def get_records_by_model(self, model_name: str) -> List[EvaluationRecord]:
        """è·å–ç‰¹å®šæ¨¡å‹çš„è®°å½•"""
        all_records = self.load_all_records()
        return [r for r in all_records if r.model_name == model_name]
    
    def get_records_by_task(self, task_type: TaskType) -> List[EvaluationRecord]:
        """è·å–ç‰¹å®šä»»åŠ¡ç±»å‹çš„è®°å½•"""
        all_records = self.load_all_records()
        return [r for r in all_records if r.task_type == task_type]


# ==============================================================================
# æŠ¥å‘Šç”Ÿæˆ
# ==============================================================================

class ReportGenerator:
    """è¯„æµ‹æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, logger: EvaluationLogger):
        self.logger = logger
    
    def generate_report(self, output_path: Optional[Path] = None) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š
        
        Args:
            output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼Œé»˜è®¤ä¸º evaluation_reports/report_{timestamp}.md
        
        Returns:
            str: æŠ¥å‘Šå†…å®¹
        """
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = REPORTS_DIR / f"evaluation_report_{timestamp}.md"
        
        records = self.logger.load_all_records()
        
        if not records:
            return "âŒ æ²¡æœ‰å¯ç”¨çš„è¯„æµ‹æ•°æ®"
        
        # ä½¿ç”¨pandasè¿›è¡Œæ•°æ®åˆ†æ
        df = pd.DataFrame([r.to_dict() for r in records])
        
        # å¼€å§‹æ„å»ºæŠ¥å‘Š
        report = []
        report.append("# æ²æ¢§AIè§£é¢˜ç³»ç»Ÿ - å¤§æ¨¡å‹å¯¹æ¯”è¯„æµ‹æŠ¥å‘Š\n")
        report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report.append(f"**è¯„æµ‹æ ·æœ¬æ•°**: {len(records)}\n")
        report.append(f"**æ¶‰åŠæ¨¡å‹æ•°**: {df['model_name'].nunique()}\n")
        report.append("\n---\n")
        
        # 1. æ€»ä½“è¡¨ç°æ¦‚è§ˆ
        report.append(self._generate_executive_summary(df))
        
        # 2. ç»¼åˆè¯„åˆ†å¯¹æ¯”è¡¨
        report.append(self._generate_comparison_table(df))
        
        # 3. åˆ†ä»»åŠ¡è¯¦ç»†åˆ†æ
        report.append(self._generate_task_analysis(df))
        
        # 4. æ¨¡å‹é—®é¢˜è®°å½•
        report.append(self._generate_issues_log(records))
        
        # 5. æœ€ç»ˆæ¨è
        report.append(self._generate_recommendation(df))
        
        # 6. é™„å½•ï¼šåŸå§‹æ•°æ®ç»Ÿè®¡
        report.append(self._generate_statistics_appendix(df))
        
        # ä¿å­˜æŠ¥å‘Š
        report_content = "\n".join(report)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… è¯„æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        return report_content
    
    def _generate_executive_summary(self, df: pd.DataFrame) -> str:
        """ç”Ÿæˆæ€»ä½“è¡¨ç°æ¦‚è§ˆ"""
        section = []
        section.append("## ğŸ“Š æ€»ä½“è¡¨ç°æ¦‚è§ˆ (Executive Summary)\n")
        
        # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„ç»¼åˆå¾—åˆ†
        score_columns = [col for col in df.columns if 'score' in col]
        
        model_scores = {}
        for model in df['model_name'].unique():
            model_df = df[df['model_name'] == model]
            avg_score = model_df[score_columns].mean().mean()
            model_scores[model] = avg_score
        
        # æ’åº
        sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)
        
        section.append(f"### ğŸ† ç»¼åˆæ’å\n")
        for rank, (model, score) in enumerate(sorted_models, 1):
            section.append(f"{rank}. **{model}** - ç»¼åˆå¾—åˆ†: {score:.2f}/5.0\n")
        
        section.append("\n### ğŸ’¡ å…³é”®å‘ç°\n")
        
        best_model = sorted_models[0][0]
        section.append(f"- **æœ€ä½³ç»¼åˆè¡¨ç°**: {best_model}\n")
        
        # æˆæœ¬åˆ†æï¼ˆåŸºäºæ¨¡å‹åç§°åˆ¤æ–­ï¼‰
        if "32b" in best_model.lower():
            section.append(f"- **æ€§ä»·æ¯”ä¼˜é€‰**: {best_model}ï¼ˆ32Bæ¨¡å‹ï¼Œæˆæœ¬ä½ï¼‰\n")
        elif "235b" in best_model.lower():
            section.append(f"- **é«˜æ€§èƒ½é€‰æ‹©**: {best_model}ï¼ˆ235Bæ¨¡å‹ï¼Œæ¨ç†èƒ½åŠ›å¼ºï¼‰\n")
        
        section.append("\n---\n")
        return "\n".join(section)
    
    def _generate_comparison_table(self, df: pd.DataFrame) -> str:
        """ç”Ÿæˆç»¼åˆè¯„åˆ†å¯¹æ¯”è¡¨"""
        section = []
        section.append("## ğŸ“ˆ ç»¼åˆè¯„åˆ†å¯¹æ¯”è¡¨\n")
        
        # å®šä¹‰è¯„åˆ†ç»´åº¦
        dimensions = {
            "é€šç”¨èƒ½åŠ›": [
                'instruction_following_score',
                'format_correction_score',
                'hallucination_score'
            ],
            "è§£é¢˜èƒ½åŠ›": [
                'ocr_accuracy_score',
                'correctness_score',
                'reasoning_quality_score'
            ],
            "æ‰¹æ”¹èƒ½åŠ›": [
                'error_detection_score',
                'explanation_clarity_score',
                'knowledge_point_accuracy_score'
            ],
            "ç”Ÿé¢˜èƒ½åŠ›": [
                'relevance_score',
                'creativity_difficulty_score',
                'answer_integrity_score'
            ]
        }
        
        # æŒ‰ä»»åŠ¡ç±»å‹ç”Ÿæˆå¯¹æ¯”è¡¨
        for task_type in ['solve', 'review', 'generate']:
            task_df = df[df['task_type'] == task_type]
            
            if len(task_df) == 0:
                continue
            
            task_name_map = {
                'solve': 'ğŸ” è§£é¢˜ä»»åŠ¡',
                'review': 'âœï¸ æ‰¹æ”¹ä»»åŠ¡',
                'generate': 'ğŸ“ ç”Ÿé¢˜ä»»åŠ¡'
            }
            
            section.append(f"### {task_name_map[task_type]}\n")
            
            # æ„å»ºè¡¨æ ¼
            table_header = "| æ¨¡å‹ | å¹³å‡åˆ† |"
            table_sep = "|------|--------|"
            
            # æ·»åŠ ç›¸å…³ç»´åº¦åˆ—
            relevant_scores = []
            if task_type == 'solve':
                relevant_scores = dimensions["é€šç”¨èƒ½åŠ›"] + dimensions["è§£é¢˜èƒ½åŠ›"]
            elif task_type == 'review':
                relevant_scores = dimensions["é€šç”¨èƒ½åŠ›"] + dimensions["æ‰¹æ”¹èƒ½åŠ›"]
            elif task_type == 'generate':
                relevant_scores = dimensions["é€šç”¨èƒ½åŠ›"] + dimensions["ç”Ÿé¢˜èƒ½åŠ›"]
            
            # ç®€åŒ–è¡¨å¤´
            score_names = {
                'instruction_following_score': 'æŒ‡ä»¤éµå¾ª',
                'format_correction_score': 'æ ¼å¼æ­£ç¡®',
                'hallucination_score': 'æ— å¹»è§‰',
                'ocr_accuracy_score': 'OCRå‡†ç¡®',
                'correctness_score': 'ç­”æ¡ˆæ­£ç¡®',
                'reasoning_quality_score': 'æ¨ç†è´¨é‡',
                'error_detection_score': 'é”™è¯¯æ£€æµ‹',
                'explanation_clarity_score': 'è§£ææ¸…æ™°',
                'knowledge_point_accuracy_score': 'çŸ¥è¯†ç‚¹å‡†ç¡®',
                'relevance_score': 'é¢˜ç›®ç›¸å…³',
                'creativity_difficulty_score': 'åˆ›æ–°éš¾åº¦',
                'answer_integrity_score': 'ç­”æ¡ˆå®Œæ•´'
            }
            
            for score in relevant_scores:
                if score in score_names:
                    table_header += f" {score_names[score]} |"
                    table_sep += "--------|"
            
            section.append(table_header)
            section.append(table_sep)
            
            # å¡«å……æ•°æ®
            for model in task_df['model_name'].unique():
                model_df = task_df[task_df['model_name'] == model]
                row = f"| {model} |"
                
                # è®¡ç®—å¹³å‡åˆ†
                avg_score = model_df[relevant_scores].mean().mean()
                row += f" **{avg_score:.2f}** |"
                
                # å„ç»´åº¦å¾—åˆ†
                for score in relevant_scores:
                    score_val = model_df[score].mean()
                    row += f" {score_val:.2f} |"
                
                section.append(row)
            
            section.append("\n")
        
        section.append("---\n")
        return "\n".join(section)
    
    def _generate_task_analysis(self, df: pd.DataFrame) -> str:
        """ç”Ÿæˆåˆ†ä»»åŠ¡è¯¦ç»†åˆ†æ"""
        section = []
        section.append("## ğŸ”¬ åˆ†ä»»åŠ¡è¯¦ç»†åˆ†æ\n")
        
        task_names = {
            'solve': 'è§£é¢˜ä»»åŠ¡',
            'review': 'æ‰¹æ”¹ä»»åŠ¡',
            'generate': 'ç”Ÿé¢˜ä»»åŠ¡'
        }
        
        for task_type in ['solve', 'review', 'generate']:
            task_df = df[df['task_type'] == task_type]
            
            if len(task_df) == 0:
                continue
            
            section.append(f"### {task_names[task_type]}\n")
            section.append(f"**æ ·æœ¬æ•°**: {len(task_df)}\n\n")
            
            # å„æ¨¡å‹è¡¨ç°
            for model in task_df['model_name'].unique():
                model_df = task_df[task_df['model_name'] == model]
                section.append(f"#### {model}\n")
                section.append(f"- æ ·æœ¬æ•°: {len(model_df)}\n")
                section.append(f"- å¹³å‡å“åº”æ—¶é—´: {model_df['response_time_seconds'].mean():.2f}ç§’\n")
                section.append(f"- å¹³å‡Tokenæ•°: {model_df['token_count'].mean():.0f}\n")
                section.append("\n")
            
            section.append("\n")
        
        section.append("---\n")
        return "\n".join(section)
    
    def _generate_issues_log(self, records: List[EvaluationRecord]) -> str:
        """ç”Ÿæˆæ¨¡å‹é—®é¢˜è®°å½•"""
        section = []
        section.append("## âš ï¸ æ¨¡å‹é—®é¢˜è®°å½• (Qualitative Issues Log)\n")
        section.append("*æ­¤éƒ¨åˆ†è®°å½•äº†æµ‹è¯•ä¸­å‘ç°çš„å…¸å‹é—®é¢˜ï¼Œå¯ä½œä¸ºPromptä¼˜åŒ–æˆ–å¾®è°ƒçš„ä¾æ®*\n\n")
        
        # æŒ‰æ¨¡å‹åˆ†ç»„
        model_issues = {}
        for record in records:
            if record.notes or record.typical_failures:
                if record.model_name not in model_issues:
                    model_issues[record.model_name] = []
                
                issue_entry = {
                    'task_type': record.task_type,
                    'notes': record.notes,
                    'failures': record.typical_failures
                }
                model_issues[record.model_name].append(issue_entry)
        
        if not model_issues:
            section.append("*æš‚æ— è®°å½•çš„é—®é¢˜*\n")
        else:
            for model, issues in model_issues.items():
                section.append(f"### {model}\n")
                
                for idx, issue in enumerate(issues, 1):
                    section.append(f"**é—®é¢˜ #{idx}** ({issue['task_type']})\n")
                    if issue['notes']:
                        section.append(f"- æè¿°: {issue['notes']}\n")
                    if issue['failures']:
                        section.append(f"- å…¸å‹å¤±è´¥:\n")
                        for failure in issue['failures']:
                            section.append(f"  - {failure}\n")
                    section.append("\n")
        
        section.append("---\n")
        return "\n".join(section)
    
    def _generate_recommendation(self, df: pd.DataFrame) -> str:
        """ç”Ÿæˆæœ€ç»ˆæ¨è"""
        section = []
        section.append("## ğŸ¯ æœ€ç»ˆæ¨è\n")
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        score_columns = [col for col in df.columns if 'score' in col]
        model_scores = {}
        for model in df['model_name'].unique():
            model_df = df[df['model_name'] == model]
            avg_score = model_df[score_columns].mean().mean()
            model_scores[model] = avg_score
        
        best_model = max(model_scores, key=model_scores.get)
        
        section.append(f"### æ¨èæ¨¡å‹: **{best_model}**\n")
        section.append(f"**ç»¼åˆå¾—åˆ†**: {model_scores[best_model]:.2f}/5.0\n\n")
        
        section.append("### æ¨èç†ç”±\n")
        
        # æ ¹æ®æ¨¡å‹åç§°æ¨æ–­ç‰¹ç‚¹
        if "32b-instruct" in best_model.lower():
            section.append("- âœ… **æŒ‡ä»¤éµå¾ªèƒ½åŠ›å¼º** (SIFOè¯„åˆ†é«˜)\n")
            section.append("- âœ… **æ€§ä»·æ¯”ä¼˜ç§€** (32Bå‚æ•°é‡ï¼Œæ¨ç†æˆæœ¬ä½)\n")
            section.append("- âœ… **é€‚åˆç”Ÿäº§éƒ¨ç½²** (ç›´æ¥æŒ‡ä»¤æ‰§è¡Œï¼Œå“åº”å¿«)\n")
        elif "32b-thinking" in best_model.lower():
            section.append("- âœ… **æ€è€ƒé“¾æ¨ç†** (é€‚åˆå¤æ‚é—®é¢˜)\n")
            section.append("- âœ… **æ€§ä»·æ¯”ä¼˜ç§€** (32Bå‚æ•°é‡)\n")
            section.append("- âš ï¸ **å“åº”å¯èƒ½è¾ƒæ…¢** (éœ€è¦æ€è€ƒæ—¶é—´)\n")
        elif "235b" in best_model.lower():
            section.append("- âœ… **é«˜æ€§èƒ½æ¨ç†** (AIME25/LCBè¯„åˆ†é«˜)\n")
            section.append("- âœ… **OCRèƒ½åŠ›å¼º** (æ–‡å­—è¯†åˆ«å‡†ç¡®)\n")
            section.append("- âš ï¸ **èµ„æºè¦æ±‚é«˜** (235Bå‚æ•°é‡ï¼Œéœ€è¦æ›´å¤šGPU)\n")
        
        section.append("\n### éƒ¨ç½²å»ºè®®\n")
        section.append("1. **å¼€å‘/æµ‹è¯•ç¯å¢ƒ**: ä½¿ç”¨32Bæ¨¡å‹ï¼ˆå¿«é€Ÿè¿­ä»£ï¼Œæˆæœ¬ä½ï¼‰\n")
        section.append("2. **ç”Ÿäº§ç¯å¢ƒ**: æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©\n")
        section.append("   - é«˜å¹¶å‘åœºæ™¯ â†’ 32B-Instruct\n")
        section.append("   - é«˜å‡†ç¡®ç‡éœ€æ±‚ â†’ 235Bç³»åˆ—\n")
        section.append("3. **å¾®è°ƒæ–¹å‘**: æ ¹æ®\"æ¨¡å‹é—®é¢˜è®°å½•\"è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–\n")
        
        section.append("\n---\n")
        return "\n".join(section)
    
    def _generate_statistics_appendix(self, df: pd.DataFrame) -> str:
        """ç”Ÿæˆç»Ÿè®¡é™„å½•"""
        section = []
        section.append("## ğŸ“ é™„å½•ï¼šåŸå§‹æ•°æ®ç»Ÿè®¡\n")
        
        section.append("### æ•°æ®æ¦‚è§ˆ\n")
        section.append(f"- æ€»æ ·æœ¬æ•°: {len(df)}\n")
        section.append(f"- æ¨¡å‹æ•°: {df['model_name'].nunique()}\n")
        section.append(f"- ä»»åŠ¡ç±»å‹åˆ†å¸ƒ:\n")
        for task_type in df['task_type'].unique():
            count = len(df[df['task_type'] == task_type])
            section.append(f"  - {task_type}: {count} ({count/len(df)*100:.1f}%)\n")
        
        section.append("\n### æ€§èƒ½ç»Ÿè®¡\n")
        section.append(f"- å¹³å‡å“åº”æ—¶é—´: {df['response_time_seconds'].mean():.2f}ç§’\n")
        section.append(f"- å¹³å‡Tokenæ•°: {df['token_count'].mean():.0f}\n")
        
        section.append("\n---\n")
        section.append("*æŠ¥å‘Šç”Ÿæˆå®Œæ¯•*\n")
        
        return "\n".join(section)


# ==============================================================================
# ä¾¿æ·å‡½æ•°
# ==============================================================================

def create_evaluation_record(
    model_name: str,
    task_type: TaskType,
    input_prompt: str,
    raw_output: str,
    input_image_path: Optional[str] = None,
    response_time: float = 0.0,
    token_count: int = 0,
    manual_scores: Optional[Dict[str, float]] = None,
    notes: str = "",
    typical_failures: Optional[List[str]] = None
) -> EvaluationRecord:
    """
    åˆ›å»ºè¯„æµ‹è®°å½•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        model_name: æ¨¡å‹åç§°
        task_type: ä»»åŠ¡ç±»å‹
        input_prompt: è¾“å…¥æç¤º
        raw_output: åŸå§‹è¾“å‡º
        input_image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
        response_time: å“åº”æ—¶é—´
        token_count: Tokenæ•°é‡
        manual_scores: äººå·¥è¯„åˆ†
        notes: å¤‡æ³¨
        typical_failures: å…¸å‹å¤±è´¥
    
    Returns:
        EvaluationRecord: è¯„æµ‹è®°å½•
    """
    # ç”Ÿæˆè®°å½•ID
    record_id = f"{model_name}_{task_type}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
    
    # è‡ªåŠ¨è¯„åˆ†
    scorer = EvaluationScorer()
    scores = scorer.score_task(raw_output, task_type, manual_scores=manual_scores)
    
    # åˆ›å»ºè®°å½•
    record = EvaluationRecord(
        record_id=record_id,
        model_name=model_name,
        task_type=task_type,
        timestamp=datetime.now().isoformat(),
        input_prompt=input_prompt,
        input_image_path=input_image_path,
        raw_output=raw_output,
        response_time_seconds=response_time,
        token_count=token_count,
        notes=notes,
        typical_failures=typical_failures or [],
        **scores
    )
    
    return record


def quick_evaluate_and_log(
    model_name: str,
    task_type: TaskType,
    input_prompt: str,
    raw_output: str,
    **kwargs
) -> None:
    """
    å¿«é€Ÿè¯„ä¼°å¹¶è®°å½•ï¼ˆä¸€æ­¥å®Œæˆï¼‰
    
    Args:
        model_name: æ¨¡å‹åç§°
        task_type: ä»»åŠ¡ç±»å‹
        input_prompt: è¾“å…¥æç¤º
        raw_output: æ¨¡å‹è¾“å‡º
        **kwargs: å…¶ä»–å‚æ•°ï¼ˆä¼ é€’ç»™create_evaluation_recordï¼‰
    """
    logger = EvaluationLogger()
    record = create_evaluation_record(
        model_name=model_name,
        task_type=task_type,
        input_prompt=input_prompt,
        raw_output=raw_output,
        **kwargs
    )
    logger.log_evaluation(record)


# ==============================================================================
# æµ‹è¯•ä»£ç 
# ==============================================================================

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("è¯„æµ‹æ¡†æ¶æµ‹è¯•")
    print("=" * 70 + "\n")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    logger = EvaluationLogger()
    
    # æ¨¡æ‹Ÿå‡ æ¡è¯„æµ‹è®°å½•
    test_models = ["qwen-vl-max", "qwen3-vl-32b-instruct", "qwen3-vl-235b-a22b-thinking"]
    
    for model in test_models:
        for task_type in ["solve", "review"]:
            quick_evaluate_and_log(
                model_name=model,
                task_type=task_type,
                input_prompt="æµ‹è¯•æç¤º",
                raw_output="æµ‹è¯•è¾“å‡ºå†…å®¹",
                notes="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è®°å½•",
                response_time=2.5,
                token_count=150
            )
    
    print(f"\nâœ… å·²ç”Ÿæˆ {len(test_models) * 2} æ¡æµ‹è¯•è®°å½•\n")
    
    # ç”ŸæˆæŠ¥å‘Š
    generator = ReportGenerator(logger)
    report = generator.generate_report()
    
    print("\nğŸ“„ æŠ¥å‘Šé¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰:")
    print(report[:500])
    print("...\n")

